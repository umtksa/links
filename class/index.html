<!DOCTYPE html>
<html>
<head>
    <title>Audio Emoji Classifier</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js"></script>
    <style>
        body {
            display: flex;
            flex-direction: column; /* Arrange items vertically */
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
            font-family: sans-serif;
        }
        #label-container {
            font-size: 10em; /* Make the emoji large */
            text-align: center;
            margin-top: 20px; /* Add some space above the emoji */
        }
        button {
            padding: 10px 20px;
            font-size: 1.2em;
            cursor: pointer;
            border: none;
            border-radius: 8px;
            background-color: #4CAF50;
            color: white;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);
            transition: background-color 0.3s ease;
        }
        button:hover {
            background-color: #45a049;
        }
    </style>
</head>
<body>
    <button type="button" onclick="init()">Start</button>

    <div id="label-container">ðŸ¤”</div> <script type="text/javascript">
        // more documentation available at
        // https://github.com/tensorflow/tfjs-models/tree/master/speech-commands

        // the link to your model provided by Teachable Machine export panel
        // IMPORTANT: Replace the placeholder below with your actual model UR

        let recognizer;
        let classLabels;
        const labelContainer = document.getElementById("label-container");
        let isListening = false; // Flag to prevent multiple starts

        async function createModel() {
            const checkpointURL = "https://raw.githubusercontent.com/umtksa/links/refs/heads/master/class/model.json"; // model topology
            const metadataURL = "https://raw.githubusercontent.com/umtksa/links/refs/heads/master/class/metadata.json"; // model metadata

            // Create the recognizer model
            recognizer = speechCommands.create(
                "BROWSER_FFT", // fourier transform type, not useful to change
                undefined, // speech commands vocabulary feature, not useful for your models
                checkpointURL,
                metadataURL);

            // check that model and metadata are loaded via HTTPS requests.
            await recognizer.ensureModelLoaded();

            return recognizer;
        }

        async function init() {
            // Prevent starting multiple times if the button is clicked rapidly
            if (isListening) {
                console.log("Already listening...");
                return;
            }
            isListening = true;

            // Create the model if it hasn't been created yet
            if (!recognizer) {
                 recognizer = await createModel();
                 classLabels = recognizer.wordLabels(); // get class labels
            }


            // The probability threshold for a class to be considered detected.
            // Adjust this value if the model is not detecting sounds reliably.
            // Lowering it might make it more sensitive but could also trigger on noise.
            // Raising it makes it less sensitive, requiring higher confidence.
            const probabilityThreshold = 0.5; // You can adjust this value

            // listen() takes two arguments:
            // 1. A callback function that is invoked anytime a word is recognized.
            // 2. A configuration object with adjustable fields
            recognizer.listen(result => {
                const scores = result.scores; // probability of prediction for each class
                // Find the class with the highest probability
                let maxScore = -Infinity;
                let maxIndex = -1;
                for (let i = 0; i < scores.length; i++) {
                    if (scores[i] > maxScore) {
                        maxScore = scores[i];
                        maxIndex = i;
                    }
                }

                const detectedClass = classLabels[maxIndex];

                // Update the emoji based on the detected class and probability
                if (maxScore > probabilityThreshold) {
                    switch (detectedClass) {
                        case "cakmak":
                            labelContainer.innerHTML = "ðŸ”¥"; // Flame emoji for cakmak
                            break;
                        case "kahve":
                            labelContainer.innerHTML = "â˜•"; // Coffee emoji for kahve
                            break;
                        case "koh":
                            labelContainer.innerHTML = "ðŸ¤§"; // Sneezing face emoji for koh
                            break;
                         case "_background_noise_":
                             labelContainer.innerHTML = "ðŸ¤«"; // Shushing face for background noise (optional)
                             break;
                        default:
                            // For any other detected class (including _unknown_ if it exceeds threshold)
                            labelContainer.innerHTML = "ðŸ¤”"; // Thinking face emoji
                            break;
                    }
                } else {
                    // If no class meets the threshold, show the thinking face
                    labelContainer.innerHTML = "ðŸ¤”";
                }

            }, {
                includeSpectrogram: false, // Set to true if you need the spectrogram data in the result object
                probabilityThreshold: probabilityThreshold, // Use the defined threshold for triggering the callback
                invokeCallbackOnNoiseAndUnknown: true, // Set to true to trigger the callback for _background_noise_ and _unknown_ classes
                overlapFactor: 0.50 // Controls overlap between audio windows. Values between 0.5 and 0.75 are common.
            });

            console.log("Listening started...");
            // You might want to disable the button here to prevent clicking again
            document.querySelector('button').disabled = true;
        }

        // The init function is now called by clicking the button instead of window.onload

    </script>
</body>
</html>
